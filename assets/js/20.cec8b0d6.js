(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{400:function(t,a,s){"use strict";s.r(a);var n=s(19),e=function(t){t.options.__data__block__={flowchart_382ee147:"st=>start: Start\nstage1=>operation: Extract text from PDF\nstage2=>operation: Split the text and save a CSV file\nstage3=>operation: Import the CSV file to Anki\ne=>end: End\n\nst->stage1->stage2->stage3->e"}},r=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p"),s("div",{staticClass:"table-of-contents"},[s("ul",[s("li",[s("a",{attrs:{href:"#start-install-pdftotext"}},[t._v("Start: Install pdftotext")])]),s("li",[s("a",{attrs:{href:"#extract-text-from-pdf"}},[t._v("Extract text from PDF")])]),s("li",[s("a",{attrs:{href:"#split-the-text-and-save-a-csv-file"}},[t._v("Split the text and save a CSV file")])]),s("li",[s("a",{attrs:{href:"#import-the-csv-file-to-anki"}},[t._v("Import the CSV file to Anki")])])])]),s("p"),t._v(" "),s("p",[t._v("The year before last, I made a repository called "),s("a",{attrs:{href:"https://github.com/HuangJiaLian/BulkImportToAnki",target:"_blank",rel:"noopener noreferrer"}},[t._v("BulkImportToAnki"),s("OutboundLink")],1),t._v("\non Github. Until tody, I found there was a "),s("a",{attrs:{href:"https://github.com/HuangJiaLian/BulkImportToAnki/issues/1",target:"_blank",rel:"noopener noreferrer"}},[t._v("issue"),s("OutboundLink")],1),t._v("\nopened on Dec 22 2018. I'm very sorry for I didn't notice this, and sorry for I\ndidn't write any documents about this repository. So this is the reason I write\nthis article.")]),t._v(" "),s("p",[t._v("Long long time ago, I got a PDF file， which contains 100 sentences for IELTS from my\nclassmate. I didn't think that's a good idea to memorize all this by reading this file directly.")]),t._v(" "),s("p",[t._v("I knew "),s("a",{attrs:{href:"https://apps.ankiweb.net/#download",target:"_blank",rel:"noopener noreferrer"}},[t._v("Anki"),s("OutboundLink")],1),t._v(" is good for memorizing.\nBut how to convert this PDF to cards thatsupport in Anki?\nHere was a solution I discovered:")]),t._v(" "),s("FlowChart",{attrs:{id:"flowchart_382ee147",code:t.$dataBlock.flowchart_382ee147,preset:"vue"}}),s("h2",{attrs:{id:"start-install-pdftotext"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#start-install-pdftotext"}},[t._v("#")]),t._v(" Start: Install pdftotext")]),t._v(" "),s("p",[t._v("I decided to use Python to deal with this. I followed "),s("a",{attrs:{href:"https://pypi.org/project/pdftotext/",target:"_blank",rel:"noopener noreferrer"}},[t._v("this"),s("OutboundLink")],1),t._v(",\nand installed "),s("code",[t._v("pdftotext")]),t._v(", which can extract text from PDF file.")]),t._v(" "),s("h2",{attrs:{id:"extract-text-from-pdf"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#extract-text-from-pdf"}},[t._v("#")]),t._v(" Extract text from PDF")]),t._v(" "),s("p",[t._v("I could extract text from my pdf file by using:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pdftotext\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Load your PDF")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"100_sentences.pdf"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rb"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pdf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pdftotext"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PDF"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Iterate over all the pages")]),t._v("\npages "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" page "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" pdf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("page"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("::: details Output")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("               7000 雅思词汇用 100 个句子记完！\n100 套真题中提炼而出的 100 百个经典句子，包涵了 7000 个雅思词汇。\n1. Typical of the grassland dwellers of the continent is the American antelope, or\npronghorn.\n1.美洲羚羊，或称叉角羚，是该大陆典型的草原动物。\n2. Of the millions who saw Haley’s comet in 1986, how many people will live long\nenough to see it return in the twenty-first century.\n2. 1986 年看见哈雷慧星的千百万人当中，有多少人能够长寿到足以目睹它在二十一世纪\n的回归呢？\n3. Anthropologists have discovered that fear, happiness, sadness, and surprise\nare universally reflected in facial expressions.\n3.人类学家们已经发现，恐惧，快乐，悲伤和惊奇都会行之于色，这在全人类是共通的。\n4. Because of its irritating effect on humans, the use of phenol as a general\nantiseptic has been largely discontinued.\n4.由于苯酚对人体带有刺激性作用，它基本上已不再被当作常用的防腐剂了。\n5. In group to remain in existence, a profit-making organization must, in the\nlong run, produce something consumers consider useful or desirable.\n5.任何盈利组织若要生存，最终都必须生产出消费者可用或需要的产品。\n6. The greater the population there is in a locality; the greater the need there is\nfor water, transportation, and disposal of refuse.\n6.一个地方的人口越多，其对水，交通和垃圾处理的需求就会越大。\n7. It is more difficult to write simply, directly, and effectively than to employ\nflowery but vague expressions that only obscure one’s meaning.\n7.简明，直接，有力的写作难于花哨，含混而意义模糊的表达。\n                                         1 / 16\n")])])]),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("8. With modern offices becoming more mechanized, designers are attempting\nto personalize them with warmer, less severe interiors.\n8.随着现代办公室的日益自动化，设计师们正试图利用较为温暖而不太严肃的内部装饰来使\n其具有亲切感。\n9. The difference between libel and slander is that libel is printed while slander\nis spoken.\n9.诽谤和流言的区别在于前者是书面的，而后者是口头的。\n10. The knee is the joints where the thigh bone meets the large bone of the\nlower leg.\n10.膝盖是大腿骨和小腿胫的连接处。\n11. Acids are chemical compounds that, in water solution, have a sharp taste, a\ncorrosive action on metals, and the ability to turn certain blue vegetable dyes\nred.\n11.酸是一种化合物，它在溶于水时具有强烈的气味和对金属的腐蚀性，并且能够使某些蓝\n色植物染料变红。\n12. Billie Holiday’s reputation as a great jazz-blues singer rests on her ability to\ngive emotional depth to her songs.\n12. Billie Holiday’s 作为一个爵士布鲁斯乐杰出歌手的名声建立在能够赋予歌曲感情深度\n的能力。\n13. Essentially, a theory is an abstract, symbolic representation of what is\nconceived to be reality.\n13.理论在本质上是对认识了的现实的一种抽象和符号化的表达。\n14. Long before children are able to speak or understand a language, they\ncommunicate through facial expressions and by making noises.\n14.儿童在能说或能听懂语言之前，很久就会通过面部表情和靠发出噪声来与人交流了。\n                                        2 / 16\n")])])]),s("p",[t._v(":::")]),t._v(" "),s("p",[t._v("Here came to the question:\n::: tip How to get the information I need?")]),t._v(" "),s("ol",[s("li",[t._v("I need all those pair sentences of Chinese and English.")]),t._v(" "),s("li",[t._v("I don't need\n"),s("ul",[s("li",[t._v("the first two lines at the begin in the first pages.")]),t._v(" "),s("li",[t._v("the numbers on the foot of each page like '1 / 16, 2 / 16'.\n:::")])])])]),t._v(" "),s("h2",{attrs:{id:"split-the-text-and-save-a-csv-file"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#split-the-text-and-save-a-csv-file"}},[t._v("#")]),t._v(" Split the text and save a CSV file")]),t._v(" "),s("p",[t._v("::: details All code")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pdftotext\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Load your PDF")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"100_sentences.pdf"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rb"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pdf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pdftotext"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PDF"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Iterate over all the pages")]),t._v("\npages "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" page "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" pdf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Remove page number")]),t._v("\n    page "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" page"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Remove all '\\n' and extra space")]),t._v("\n    page "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" page"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pages "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" page\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" re \ntext "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'[0-9]+\\.'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pages"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\npairs_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ncounter "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(sentence[i])")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pairs_list.append((li[i-1].replace('\\n',''),li[i].replace('\\n','')))")]),t._v("\n        pairs_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    counter "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" \n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pair "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" pairs_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pair"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd \ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pairs_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Front'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Back'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(df)")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'DataExportToAnki.csv'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf-8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# It's not elegant I knew, but it worked.")]),t._v("\n")])])]),s("p",[t._v(":::")]),t._v(" "),s("p",[t._v("So that , I converted the PDF to a csv file, which could be import to Anki easily.")]),t._v(" "),s("h2",{attrs:{id:"import-the-csv-file-to-anki"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#import-the-csv-file-to-anki"}},[t._v("#")]),t._v(" Import the CSV file to Anki")]),t._v(" "),s("p",[t._v("In my computer, I opened "),s("code",[t._v("Anki-> File -> Import")]),t._v(", and then chose the csv file I made.\nBecause the first colum was number field, so I ignored Field 1.\n"),s("Cimg",{attrs:{src:"https://raw.githubusercontent.com/HuangJiaLian/DataBase0/master/uPic/BWmHqE.jpg",width:"70%",caption:"Import csv to Anki"}})],1),t._v(" "),s("p",[t._v("I'm not sure whether or not a csv file could be imported to the mobile Anki directly.\nEven so, you can export a package using "),s("code",[t._v("Anki-> File -> Export")]),t._v(", and import to your phone.")]),t._v(" "),s("p",[t._v("Thank for reading.  If you have any questions, leave comments below.")])],1)}),[],!1,null,null,null);"function"==typeof e&&e(r);a.default=r.exports}}]);